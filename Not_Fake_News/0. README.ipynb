{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font color='ff4b0d'> Iowa Man or Florida Man? </font> </center>\n",
    "## <center> Training a Classifier to Distinguish Between Posts in r/news and r/nottheonion </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/reddit_logo.png\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:\n",
    "\n",
    "1. Problem Statement\n",
    "2. Data Collection and Cleaning\n",
    "3. Modeling\n",
    "4. Evaluation\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "With the rise of social media also came an increasing demand from media outlets to capture the reader's attention amid the noise of the internet. The best way online publications found to do this was to perfect the art of the punchy headline, even if at the risk of compromising their journalistic integrity. Concurrent with this phenomenon is the public preoccupation with \"fake news,\" which came to a head during the 2016 elections. Soon after these elections, a flurry of reports from various sources identified miriad ways the electorate could have been influenced by fake news and sensationalist articles coming from major news outlets. Both of these types of news (fake and/or misleading) depend on \"clickbaiting\" or eye-catching, sensationalist headlines designed to distract the reader in hopes of luring them to their websites.\n",
    "\n",
    "Natural Language Processing methods can provide readers a way to  these trap. In this data experiment, we will try to create a classifier that can distinguish between two different types of headlines. In order to do this, we will look at headlines posted in two subreddits: r/news and r/nottheonion. r/news provides subscribers with real news articles shared by users of the forum. r/nottheonion, on the other hand, only contains those news items which \"are so mind-blowingly ridiculous that you could have sworn they were from The Onion.\" \n",
    "\n",
    "The distinction between the two is palpable for human readers since headlines from the latter subreddit tends to elicit a sense of amusement given their supposed ridiculousness. To a classifier, however, this distinction may not be so clear, especially since subreddits both contain real news and may show some overlap. This subtle difference makes these two subreddits ideal for the tasks at hand which are: (1) to get a sense of how effective natural language processing tools are at distinguishing different types of news headlines; and (2) see if our classifier can help us identify structural differences between the two classes of headlines in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Cleaning\n",
    "\n",
    "The data were collected via Pushshift.io. One thousand posts were pulled from each subreddit. Duplicates were removed and replaced with new headlines. The headlines were placed into a Pandas DataFrame and labeled by source (0 for r/news, the positive class, and 1 for r/nottheonion, the negative class). The resulting DataFrames were merged and pickled for the next steps. \n",
    "\n",
    "(Note: Although it is convention to label the positive class with a 1, this was convention was not followed in this case.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Preprocessing the data involved categorizing them into dependent (X, titles) and independent (y, source) variables and passed through a test-train-split. After that, four gridsearches were conducted via a pipeline. The four pipelines consisted of: (1) a CountVectorizer with a Logistic Regression; (2) a CountVectorizer with a Multinomial Naive Bayes Classifier; (3) a TF-IDF Vectorizer with a Multinomial Naive Bayes Classifier; and (4) a TF-IDF Vectorizer with a Logistic Regression. Below are the results of each: \n",
    "\n",
    "| Model | Training Accuracy |  Testing Accuracy \n",
    "|------|------|------|\n",
    "|CountVectorizer/LR |0.97|0.78|\n",
    "|------|------|------|\n",
    "|CountVectorizer/NB |0.92|0.79|\n",
    "|------|------|------|\n",
    "|TF-IDF/NB |0.93|0.77|\n",
    "|------|------|------|\n",
    "|TF-IDF/LR |0.99|0.78|\n",
    "\n",
    "Parameters fed to the gridsearch included: max_words, max_features, n_gram range, lr_penalty (for logistic regressions), and strip_accents (for TF-IDF). Overall, changes to parameters showed very little change to performance of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The CountVectorizer with a Multinomial Naive Bayes Classifier gave the highest accuracy on the testing data while showing the least amount of overfit with the training data. This is the model that will be used to evaluate our two questions. \n",
    "\n",
    "#### How Well Did the Classifier Do Given these Data? \n",
    "\n",
    "The CountVectorizer/Multinomial Naive Bayes combination did surprisingly well given the similarities between the corpuses, demonstrating a prediction accuracy of 79% against the testing data. This means that about 4 out of 5 headlines were correctly classified into the r/news or r/nottheonion subreddits. Some of the subtle differences being caught by the classifiers can be explained by the word frequencies in each corpus: \n",
    "\n",
    "*Word Frequencies in r/news:*\n",
    "<img src=\"./images/news_words.png\" width=\"600\"/>\n",
    "\n",
    "*Word Frequencies in r/nottheonion:*\n",
    "\n",
    "<img src=\"./images/nottheonion_words.png\" width=\"600\"/>\n",
    "\n",
    "#### Where did the model struggle?\n",
    "\n",
    "<img src=\"./images/confusion_matrix.png\" width=\"600\"/>\n",
    "\n",
    "#### What can be improved?\n",
    "\n",
    "The model can be improved by adopting more a more granular approach in the data cleaning and preprocessing stages. For instance, the based on the results of the confusion matrix, the model will stand to benefit from the removal of non-ASCII headlines. The corpuses can also be normalized via lemmatization or stemming in order to see if this will bring surface other frequent words. Finally, more work can be done to identify stopwords common in news headlines so as to eliminate some noise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Training a classifier to identify patterns in news headlines can be the first step in identifying “fake news” or building a “clickbait” filter. This can go a long way in restructuring the way information is distributed and consumed in this new, profit driven era of content creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
